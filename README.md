# Self-Supervised Learning for Anomaly Detection

## Introduction
This project thesis aims to implement an Anomaly Detection framework using a Self-Supervised approach.


Self-supervised learning (SSL) is a subcategory of unsupervised learning. This method can achieve an excellent performance comparable to the fully-supervised baselines in several challenging tasks such as visual representation learning, object detection, and natural language processing.


SSL learns a generalizable representation from unlabelled data by solving a supervised proxy task (called PRETEXT TASK) which is often unrelated to the target task, but can help the network to learn a better representation.<br />
Pretext task examples:
* Rotation classification
* jigsaw puzzle
* denoising

The Pretext Task is solved creating an artificial dataset based on the original unlabelled dataset, where the model is trained to solve a problem in a supervised approach using pseudo labels (autogenerated during the dataset preparation, no human manual labeling).<br />
After training the network, the knowledge (weights) are transferred in a new model (equal or smaller) to solve the so-called Downstream Task (the original problem we wanted to solve), using the original real data.

## Personal Approach
This SSL approach is inspired by "CutPaste: Self-Supervised Learning for Anomaly Detection and Localization", where the pretext task is a classification task over an artificial dataset on which they are applied image transformations.<br />
The pretext task dataset is generated in such a way that we have 3 classes, where:
* 0 -> good image
* 1 -> cut-paste image (an image where a random portion of it is cut and pasted in a random location)
* 2 -> scar (a random colored line is drawn over the image in a random position)

The goal is to generate artificial anomalies and train the model to recognize those representations for the downstream task (Anomaly detection)

The dataset used is the MVTec dataset, a widely used dataset for anomaly detection benchmark.<br />
The personal contribution aims to create a Generative Dataset in the Pretext Task, where the dataset is re-generated every N training epochs. This choice is jusyfied by the fact the training images for each MVTec object class are very few (especially the toothbrush one). This choice should avoid the overfit problem (at least partially) and bring more generalization to the model
